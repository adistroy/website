"use strict";(self.webpackChunkkubedl_website=self.webpackChunkkubedl_website||[]).push([[222],{3905:function(e,n,r){r.d(n,{Zo:function(){return m},kt:function(){return u}});var t=r(7294);function o(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function a(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach((function(n){o(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function l(e,n){if(null==e)return{};var r,t,o=function(e,n){if(null==e)return{};var r,t,o={},i=Object.keys(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||(o[r]=e[r]);return o}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var s=t.createContext({}),c=function(e){var n=t.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):a(a({},n),e)),r},m=function(e){var n=c(e.components);return t.createElement(s.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},d=t.forwardRef((function(e,n){var r=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),d=c(r),u=o,f=d["".concat(s,".").concat(u)]||d[u]||p[u]||i;return r?t.createElement(f,a(a({ref:n},m),{},{components:r})):t.createElement(f,a({ref:n},m))}));function u(e,n){var r=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var i=r.length,a=new Array(i);a[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,a[1]=l;for(var c=2;c<i;c++)a[c]=r[c];return t.createElement.apply(null,a)}return t.createElement.apply(null,r)}d.displayName="MDXCreateElement"},2594:function(e,n,r){r.r(n),r.d(n,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return c},toc:function(){return m},default:function(){return d}});var t=r(7462),o=r(3366),i=(r(7294),r(3905)),a=["components"],l={sidebar_position:1},s="Introduction",c={unversionedId:"serving/intro",id:"serving/intro",title:"Introduction",description:"KubeDL Serving provides a group of user freindly APIs to construct online model inference services. It closely cooperates with training and model stages, making end-to-end deep learning development automatically.",source:"@site/docs/serving/intro.md",sourceDirName:"serving",slug:"/serving/intro",permalink:"/docs/serving/intro",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/serving/intro.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Backing Storage",permalink:"/docs/model/storage"},next:{title:"Design",permalink:"/docs/serving/design"}},m=[{value:"Inference With Single Model",id:"inference-with-single-model",children:[],level:2},{value:"Inference With Multiple Models",id:"inference-with-multiple-models",children:[],level:2}],p={toc:m};function d(e){var n=e.components,r=(0,o.Z)(e,a);return(0,i.kt)("wrapper",(0,t.Z)({},p,r,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"KubeDL Serving provides a group of user freindly APIs to construct online model inference services. It closely cooperates with ",(0,i.kt)("inlineCode",{parentName:"p"},"training")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"model")," stages, making end-to-end deep learning development automatically."),(0,i.kt)("p",null,"KubeDL provides CRD ",(0,i.kt)("inlineCode",{parentName:"p"},"Inference")," to accomplish this:"),(0,i.kt)("h2",{id:"inference-with-single-model"},"Inference With Single Model"),(0,i.kt)("p",null,"Inference describes an expeced inference service including adopted framework, predictor templates, autoscaling polices... An example YAML looks like below, this example shows how inference service serves single model:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: serving.kubedl.io/v1alpha1\nkind: Inference\nmetadata:\n  name: hello-inference\nspec:\n  framework: TFServing\n  predictors:\n  - name: model-predictor\n    modelVersion: model\n    replicas: 3\n    autoScale:\n      minReplicas: 1\n      maxReplicas: 10\n    batching:\n      batchSize: 32\n    template:\n      spec:\n        containers:\n        - name: tensorflow\n          args:\n          - --port=9000\n          - --rest_api_port=8500\n          - --model_name=mnist\n          - --model_base_path=/kubedl-model/\n          command:\n          - /usr/bin/tensorflow_model_server\n          image: tensorflow/serving:1.11.1\n          imagePullPolicy: IfNotPresent\n          ports:\n          - containerPort: 9000\n          - containerPort: 8500\n          resources:\n            limits:\n              cpu: 2048m\n              memory: 2Gi\n            requests:\n              cpu: 1024m\n              memory: 1Gi\n")),(0,i.kt)("h2",{id:"inference-with-multiple-models"},"Inference With Multiple Models"),(0,i.kt)("p",null,"Inference is able to serve multiple models simultaneously, which may appers in serving different model versions and takes A/B tests, for example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: serving.kubedl.io/v1alpha1\nkind: Inference\nmetadata:\n  name: hello-inference\nspec:\n  framework: TFServing\n  predictors:\n  - name: model-a-predictor\n    modelVersion: model-a\n    replicas: 3\n    trafficPercentage: 90  # 90% traffic will be roted to this predictor.\n    autoScale:\n      minReplicas: 1\n      maxReplicas: 10\n    batching:\n      batchSize: 32\n    template:\n      spec:\n        containers:\n        - name: tensorflow\n          args:\n          - --port=9000\n          - --rest_api_port=8500\n          - --model_name=mnist\n          - --model_base_path=/kubedl-model/\n          command:\n          - /usr/bin/tensorflow_model_server\n          image: tensorflow/serving:1.11.0\n          imagePullPolicy: IfNotPresent\n          ports:\n          - containerPort: 9000\n          - containerPort: 8500\n          resources:\n            limits:\n              cpu: 2048m\n              memory: 2Gi\n            requests:\n              cpu: 1024m\n              memory: 1Gi\n  - name: model-b-predictor\n    modelVersion: model-b\n    replicas: 3\n    trafficPercentage: 10  # 10% traffic will be roted to this predictor.\n    autoScale:\n      minReplicas: 1\n      maxReplicas: 10\n    batching:\n      batchSize: 64\n    template:\n      spec:\n        containers:\n        - name: tensorflow\n          args:\n          - --port=9000\n          - --rest_api_port=8500\n          - --model_name=mnist\n          - --model_base_path=/kubedl-model/\n          command:\n          - /usr/bin/tensorflow_model_server\n          image: tensorflow/serving:1.11.1\n          imagePullPolicy: IfNotPresent\n          ports:\n          - containerPort: 9000\n          - containerPort: 8500\n          resources:\n            limits:\n              cpu: 2048m\n              memory: 2Gi\n            requests:\n              cpu: 1024m\n              memory: 1Gi\n")))}d.isMDXComponent=!0}}]);